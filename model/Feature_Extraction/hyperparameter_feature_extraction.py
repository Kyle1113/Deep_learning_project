# -*- coding: utf-8 -*-
"""HyperParameter_feature_extraction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JoIKeFH1dy4VOxBd4TVRpuusYFqk9PHF

### 데이터 구축
"""

# mount Google Drive
from google.colab import drive
drive.mount('/content/gdrive')

import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.keras import Input
from tensorflow.keras.models import Model
from tensorflow.keras.applications import VGG16, VGG19
from tensorflow.keras.applications import ResNet50
from keras.layers import Conv2D, GlobalAveragePooling2D, Dense, Flatten, Dropout, BatchNormalization, Activation, MaxPooling2D
from keras.preprocessing.image import ImageDataGenerator
import pandas as pd
import json
import pandas as pd
import os
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
import zipfile
import shutil
from keras.optimizers import Adam
# from unidecode import unidecode

# 주어진 경로들
zip_file_path = '/content/gdrive/MyDrive/Colab Notebooks/딥러닝/dl_project/data/crop_data_origin.zip'
extract_path = '/content/images'
target_folder = 'crop_data_origin/top_crop_image_origin'

# 압축 파일 해제
with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

# target_folder 내부 의류 카테고리 폴더까지 접근, 각 폴더 안의 이미지 파일 개수 반환
top_path = os.path.join(extract_path, target_folder)
folders = os.listdir(top_path)

folder_info = {}

for folder in folders:
    folder_path = os.path.join(top_path, folder)
    if os.path.isdir(folder_path):
        files = os.listdir(folder_path)
        file_count = len(files)
        folder_info[folder] = file_count

# 결과 출력
for folder, count in folder_info.items():
    print(f"Folder: {folder}, Number of files: {count}")

# 데이터 저장 리스트
data = []

# JSON 파일을 처리하는 함수
def process_json_file(file_path, image_directory):
    with open(file_path, 'r', encoding='utf-8') as file:
        annotation_data = json.load(file)

        image_info = annotation_data["이미지 정보"]
        label_info = annotation_data["데이터셋 정보"]["데이터셋 상세설명"]["라벨링"]

        # '색상' 및 '카테고리' 키 유효성 검사
        if "색상" not in label_info["상의"][0] or "카테고리" not in label_info["상의"][0]:
            print(f"파일에 필요한 키가 없습니다: {file_path}. 해당 항목을 건너뜁니다.")
            return

        file_path = os.path.join(image_directory, '%d.jpg' % image_info['이미지 식별자'])
        style = label_info["상의"][0]["카테고리"]
        color = label_info["상의"][0]['색상']

        data.append({
            "파일경로": file_path,
            "style": style,
            "color": color,
        })

# top label, image 폴더 경로
top_label_path = '/content/images/crop_data_origin/label/top'
top_image_path = "/content/images/crop_data_origin/top_crop_image_origin"
# label 폴더들
categories = ["blaus_256", "neatwear_256", "shirts_256", "tshirts_256", "top_256"]

# 각 폴더에서 2000개의 JSON 파일을 랜덤으로 추출하여 처리
for category in categories:
    category_path = os.path.join(top_label_path, category)
    json_files = [f for f in os.listdir(category_path) if f.endswith('.json')]
    modified_category = category[:-4]
    image_directory = os.path.join(top_image_path, modified_category)

    for json_file in json_files:
        process_json_file(os.path.join(category_path, json_file), image_directory)

# 데이터 프레임 생성
df = pd.DataFrame(data)

# 라벨 인코딩
label_encoder_style = LabelEncoder()
label_encoder_color = LabelEncoder()

df['style'] = label_encoder_style.fit_transform(df['style'])
df['color'] = label_encoder_color.fit_transform(df['color'])

# 훈련 및 테스트 데이터 분할
train_df = df.sample(frac=0.8, random_state=42)
test_df = df.drop(train_df.index)

# 결과 출력 (예: 상위 5개 행 출력)
print(train_df.head())
print(test_df.head())

print(len(train_df))
print(len(test_df))
# 110개 정도 날라감 - 사소해

df

df['style'].unique()

# 아직 데이터 증대 활용 x, 결과가 overfitting이라면 데이터 증대 이용
train_datagen = ImageDataGenerator(rescale = 1./255)
test_datagen = ImageDataGenerator(rescale = 1./255)

batch_size = 64

training_set = train_datagen.flow_from_dataframe(train_df,
                                                 target_size = (256, 256),
                                                 x_col='파일경로',
                                                 y_col=['style','color'],
                                                 batch_size = batch_size,
                                                 class_mode = 'multi_output'
                                                 )

test_set = test_datagen.flow_from_dataframe(test_df,
                                            target_size = (256, 256),
                                            x_col='파일경로',
                                            y_col=['style','color'],
                                            batch_size = batch_size,
                                            class_mode = 'multi_output')

"""### trial 1 : epoch 결정(early stoping 사용 여부)"""

# early stopping 사용
vgg_model = VGG16(weights='imagenet', input_shape=(256,256,3), include_top=False)
vgg_model.trainable = False
x = vgg_model.get_layer('block5_pool').output

x = layers.Flatten(name='new_flatten')(x)
branch_a=layers.Dense(units=256)(x)
branch_a=layers.BatchNormalization()(branch_a)
branch_a=layers.Activation('relu')(branch_a)
branch_a=layers.Dropout(0.5)(branch_a)
branch_a=layers.Dense(units=128)(branch_a)
branch_a=layers.BatchNormalization()(branch_a)
branch_a=layers.Dropout(0.5)(branch_a)

style_prediction = layers.Dense(7, activation='softmax', name='style')(branch_a)
color_prediction = layers.Dense(21, activation='softmax', name='color')(branch_a)

model = Model(vgg_model.input,[style_prediction, color_prediction])
model.compile(optimizer=Adam(learning_rate = 3e-4), loss={'style':'sparse_categorical_crossentropy','color':'sparse_categorical_crossentropy'}, metrics=['accuracy'])

checkpoint_callback = ModelCheckpoint(
    filepath='feature_extraction_VGG_2.h5',
    monitor='val_loss',
    save_best_only=True,
    mode='min',
    verbose=1
)

early_stopping_callback = EarlyStopping(
    monitor='val_loss',
    patience=10,
    mode='min',
    verbose=1
)


history1=model.fit_generator(training_set,
                         epochs = 100,
                         validation_data = test_set,
                         callbacks=[checkpoint_callback, early_stopping_callback],
                         steps_per_epoch=len(training_set)/batch_size,
                         validation_steps=len(test_set)/batch_size
                        )

# model.save_weights('/content/gdrive/MyDrive/Colab Notebooks/딥러닝/dl_project/feature_extraction_hyperParemeter_earlyStop.h5')

import matplotlib.pyplot as plt

# 학습 과정에서의 손실을 시각화하는 함수
def plot_loss(history):
    plt.figure(figsize=(12, 6))
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Training and Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

# 학습 과정에서의 스타일 예측 정확도를 시각화하는 함수
def plot_style_accuracy(history):
    plt.figure(figsize=(12, 6))
    plt.plot(history.history['style_accuracy'], label='Training Style Accuracy')
    plt.plot(history.history['val_style_accuracy'], label='Validation Style Accuracy')
    plt.title('Training and Validation Style Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.show()

# 학습 과정에서의 색상 예측 정확도를 시각화하는 함수
def plot_color_accuracy(history):
    plt.figure(figsize=(12, 6))
    plt.plot(history.history['color_accuracy'], label='Training Color Accuracy')
    plt.plot(history.history['val_color_accuracy'], label='Validation Color Accuracy')
    plt.title('Training and Validation Color Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.show()

# 손실 시각화
plot_loss(history1)

# 스타일 예측 정확도 시각화
plot_style_accuracy(history1)

# 색상 예측 정확도 시각화
plot_color_accuracy(history1)

# early stopping 사용 x

vgg_model = VGG16(weights='imagenet', input_shape=(256,256,3), include_top=False)
vgg_model.trainable = False
x = vgg_model.get_layer('block5_pool').output

x = layers.Flatten(name='new_flatten')(x)
branch_a=layers.Dense(units=256)(x)
branch_a=layers.BatchNormalization()(branch_a)
branch_a=layers.Activation('relu')(branch_a)
branch_a=layers.Dropout(0.5)(branch_a)
branch_a=layers.Dense(units=128)(branch_a)
branch_a=layers.BatchNormalization()(branch_a)
branch_a=layers.Dropout(0.5)(branch_a)

style_prediction = layers.Dense(7, activation='softmax', name='style')(branch_a)
color_prediction = layers.Dense(21, activation='softmax', name='color')(branch_a)

model = Model(vgg_model.input,[style_prediction, color_prediction])
model.compile(optimizer=Adam(learning_rate = 3e-4), loss={'style':'sparse_categorical_crossentropy','color':'sparse_categorical_crossentropy'}, metrics=['accuracy'])

checkpoint_callback = ModelCheckpoint(
    filepath='feature_extraction_VGG_2.h5',
    monitor='val_loss',
    save_best_only=True,
    mode='min',
    verbose=1
)

history1=model.fit_generator(training_set,
                         epochs = 100,
                         validation_data = test_set,
                         callbacks=[checkpoint_callback],
                         steps_per_epoch=len(training_set)/batch_size,
                         validation_steps=len(test_set)/batch_size
                        )

# model.save_weights('/content/gdrive/MyDrive/Colab Notebooks/딥러닝/dl_project/feature_extraction_hyperParameter_noEarlyStop.h5')

import matplotlib.pyplot as plt

# 학습 과정에서의 손실을 시각화하는 함수
def plot_loss(history):
    plt.figure(figsize=(12, 6))
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Training and Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

# 학습 과정에서의 스타일 예측 정확도를 시각화하는 함수
def plot_style_accuracy(history):
    plt.figure(figsize=(12, 6))
    plt.plot(history.history['style_accuracy'], label='Training Style Accuracy')
    plt.plot(history.history['val_style_accuracy'], label='Validation Style Accuracy')
    plt.title('Training and Validation Style Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.show()

# 학습 과정에서의 색상 예측 정확도를 시각화하는 함수
def plot_color_accuracy(history):
    plt.figure(figsize=(12, 6))
    plt.plot(history.history['color_accuracy'], label='Training Color Accuracy')
    plt.plot(history.history['val_color_accuracy'], label='Validation Color Accuracy')
    plt.title('Training and Validation Color Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.show()

# 손실 시각화
plot_loss(history1)

# 스타일 예측 정확도 시각화
plot_style_accuracy(history1)

# 색상 예측 정확도 시각화
plot_color_accuracy(history1)

"""### 번외 : backbone을 VGG19 사용"""

# backbone을 vgg19 사용

vgg_model = VGG19(weights='imagenet', input_shape=(256,256,3), include_top=False)
vgg_model.trainable = False
x = vgg_model.get_layer('block5_pool').output

x = layers.Flatten(name='new_flatten')(x)
branch_a=layers.Dense(units=256)(x)
branch_a=layers.BatchNormalization()(branch_a)
branch_a=layers.Activation('relu')(branch_a)
branch_a=layers.Dropout(0.5)(branch_a)
branch_a=layers.Dense(units=128)(branch_a)
branch_a=layers.BatchNormalization()(branch_a)
branch_a=layers.Dropout(0.5)(branch_a)

style_prediction = layers.Dense(7, activation='softmax', name='style')(branch_a)
color_prediction = layers.Dense(21, activation='softmax', name='color')(branch_a)

model = Model(vgg_model.input,[style_prediction, color_prediction])
model.compile(optimizer=Adam(learning_rate = 3e-4), loss={'style':'sparse_categorical_crossentropy','color':'sparse_categorical_crossentropy'}, metrics=['accuracy'])

checkpoint_callback = ModelCheckpoint(
    filepath='feature_extraction_VGG_2.h5',
    monitor='val_loss',
    save_best_only=True,
    mode='min',
    verbose=1
)

history1=model.fit_generator(training_set,
                         epochs = 100,
                         validation_data = test_set,
                         callbacks=[checkpoint_callback],
                         steps_per_epoch=len(training_set)/batch_size,
                         validation_steps=len(test_set)/batch_size
                        )

# model.save_weights('/content/gdrive/MyDrive/Colab Notebooks/딥러닝/dl_project/feature_extraction_hyperParameter_noEarlyStop.h5')

import matplotlib.pyplot as plt

# 학습 과정에서의 손실을 시각화하는 함수
def plot_loss(history):
    plt.figure(figsize=(12, 6))
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Training and Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

# 학습 과정에서의 스타일 예측 정확도를 시각화하는 함수
def plot_style_accuracy(history):
    plt.figure(figsize=(12, 6))
    plt.plot(history.history['style_accuracy'], label='Training Style Accuracy')
    plt.plot(history.history['val_style_accuracy'], label='Validation Style Accuracy')
    plt.title('Training and Validation Style Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.show()

# 학습 과정에서의 색상 예측 정확도를 시각화하는 함수
def plot_color_accuracy(history):
    plt.figure(figsize=(12, 6))
    plt.plot(history.history['color_accuracy'], label='Training Color Accuracy')
    plt.plot(history.history['val_color_accuracy'], label='Validation Color Accuracy')
    plt.title('Training and Validation Color Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.show()

# 손실 시각화
plot_loss(history1)

# 스타일 예측 정확도 시각화
plot_style_accuracy(history1)

# 색상 예측 정확도 시각화
plot_color_accuracy(history1)

"""### trial 2 : finetune VS end to end"""

# finetune : 앞에 6개 레이어는 고정, 뒤에 나머지 7개 학습
# 코드는 없지만 학습 시키는 layer 수를 줄여봤지만 크게 의미가 없음, 학습시키는 레이어를 크게 하는 건 end to end와 다를 게 없으므로 안해 봄
vgg_model = VGG16(weights='imagenet', input_shape=(256,256,3), include_top=False)
vgg_model.trainable = True

for layer in vgg_model.layers[:7]:
  layer.trainable = False

x = vgg_model.get_layer('block5_pool').output

x = layers.Flatten(name='new_flatten')(x)
branch_a=layers.Dense(units=256)(x)
branch_a=layers.BatchNormalization()(branch_a)
branch_a=layers.Activation('relu')(branch_a)
branch_a=layers.Dropout(0.5)(branch_a)
branch_a=layers.Dense(units=128)(branch_a)
branch_a=layers.BatchNormalization()(branch_a)
branch_a=layers.Dropout(0.5)(branch_a)

style_prediction = layers.Dense(7, activation='softmax', name='style')(branch_a)
color_prediction = layers.Dense(21, activation='softmax', name='color')(branch_a)

model = Model(vgg_model.input,[style_prediction, color_prediction])
model.compile(optimizer=Adam(learning_rate = 3e-4), loss={'style':'sparse_categorical_crossentropy','color':'sparse_categorical_crossentropy'}, metrics=['accuracy'])

checkpoint_callback = ModelCheckpoint(
    filepath='feature_extraction_VGG_2.h5',
    monitor='val_loss',
    save_best_only=True,
    mode='min',
    verbose=1
)

history1=model.fit_generator(training_set,
                         epochs = 100,
                         validation_data = test_set,
                         callbacks=[checkpoint_callback],
                         steps_per_epoch=len(training_set)/batch_size,
                         validation_steps=len(test_set)/batch_size
                        )

# model.save_weights('/content/gdrive/MyDrive/Colab Notebooks/딥러닝/dl_project/feature_extraction_hyperParameter_noEarlyStop.h5')

import matplotlib.pyplot as plt

# 학습 과정에서의 손실을 시각화하는 함수
def plot_loss(history):
    plt.figure(figsize=(12, 6))
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Training and Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

# 학습 과정에서의 스타일 예측 정확도를 시각화하는 함수
def plot_style_accuracy(history):
    plt.figure(figsize=(12, 6))
    plt.plot(history.history['style_accuracy'], label='Training Style Accuracy')
    plt.plot(history.history['val_style_accuracy'], label='Validation Style Accuracy')
    plt.title('Training and Validation Style Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.show()

# 학습 과정에서의 색상 예측 정확도를 시각화하는 함수
def plot_color_accuracy(history):
    plt.figure(figsize=(12, 6))
    plt.plot(history.history['color_accuracy'], label='Training Color Accuracy')
    plt.plot(history.history['val_color_accuracy'], label='Validation Color Accuracy')
    plt.title('Training and Validation Color Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.show()

# 손실 시각화
plot_loss(history1)

# 스타일 예측 정확도 시각화
plot_style_accuracy(history1)

# 색상 예측 정확도 시각화
plot_color_accuracy(history1)

# end to end
vgg_model = VGG16(weights=None, input_shape=(256,256,3), include_top=False)
vgg_model.trainable = True

x = vgg_model.get_layer('block5_pool').output

x = layers.Flatten(name='new_flatten')(x)
branch_a=layers.Dense(units=256)(x)
branch_a=layers.BatchNormalization()(branch_a)
branch_a=layers.Activation('relu')(branch_a)
branch_a=layers.Dropout(0.5)(branch_a)
branch_a=layers.Dense(units=128)(branch_a)
branch_a=layers.BatchNormalization()(branch_a)
branch_a=layers.Dropout(0.5)(branch_a)

style_prediction = layers.Dense(7, activation='softmax', name='style')(branch_a)
color_prediction = layers.Dense(21, activation='softmax', name='color')(branch_a)

model = Model(vgg_model.input,[style_prediction, color_prediction])
model.compile(optimizer=Adam(learning_rate = 3e-4), loss={'style':'sparse_categorical_crossentropy','color':'sparse_categorical_crossentropy'}, metrics=['accuracy'])

checkpoint_callback = ModelCheckpoint(
    filepath='feature_extraction_VGG_2.h5',
    monitor='val_loss',
    save_best_only=True,
    mode='min',
    verbose=1
)

history1=model.fit_generator(training_set,
                         epochs = 100,
                         validation_data = test_set,
                         callbacks=[checkpoint_callback],
                         steps_per_epoch=len(training_set)/batch_size,
                         validation_steps=len(test_set)/batch_size
                        )

# model.save_weights('/content/gdrive/MyDrive/Colab Notebooks/딥러닝/dl_project/feature_extraction_hyperParameter_noEarlyStop.h5')

import matplotlib.pyplot as plt

# 학습 과정에서의 손실을 시각화하는 함수
def plot_loss(history):
    plt.figure(figsize=(12, 6))
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Training and Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

# 학습 과정에서의 스타일 예측 정확도를 시각화하는 함수
def plot_style_accuracy(history):
    plt.figure(figsize=(12, 6))
    plt.plot(history.history['style_accuracy'], label='Training Style Accuracy')
    plt.plot(history.history['val_style_accuracy'], label='Validation Style Accuracy')
    plt.title('Training and Validation Style Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.show()

# 학습 과정에서의 색상 예측 정확도를 시각화하는 함수
def plot_color_accuracy(history):
    plt.figure(figsize=(12, 6))
    plt.plot(history.history['color_accuracy'], label='Training Color Accuracy')
    plt.plot(history.history['val_color_accuracy'], label='Validation Color Accuracy')
    plt.title('Training and Validation Color Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.show()

# 손실 시각화
plot_loss(history1)

# 스타일 예측 정확도 시각화
plot_style_accuracy(history1)

# 색상 예측 정확도 시각화
plot_color_accuracy(history1)



"""### trial 3 : 학습률"""

# 학습률 초기값 0.01
vgg_model = VGG16(weights='imagenet', input_shape=(256,256,3), include_top=False)
vgg_model.trainable = False
x = vgg_model.get_layer('block5_pool').output

x = layers.Flatten(name='new_flatten')(x)
branch_a=layers.Dense(units=256)(x)
branch_a=layers.BatchNormalization()(branch_a)
branch_a=layers.Activation('relu')(branch_a)
branch_a=layers.Dropout(0.5)(branch_a)
branch_a=layers.Dense(units=128)(branch_a)
branch_a=layers.BatchNormalization()(branch_a)
branch_a=layers.Dropout(0.5)(branch_a)

style_prediction = layers.Dense(7, activation='softmax', name='style')(branch_a)
color_prediction = layers.Dense(21, activation='softmax', name='color')(branch_a)

model = Model(vgg_model.input,[style_prediction, color_prediction])
model.compile(optimizer=Adam(learning_rate = 0.01), loss={'style':'sparse_categorical_crossentropy','color':'sparse_categorical_crossentropy'}, metrics=['accuracy'])

checkpoint_callback = ModelCheckpoint(
    filepath='feature_extraction_VGG_2.h5',
    monitor='val_loss',
    save_best_only=True,
    mode='min',
    verbose=1
)

# early_stopping_callback = EarlyStopping(
#     monitor='val_loss',
#     patience=10,
#     mode='min',
#     verbose=1
# )

# reduce_learning_rate = ReduceLROnPlateau(
#     monitor='val_loss', factor=0.1, patience=10, verbose=1, mode='auto',
#     min_delta=0.0001, cooldown=0, min_lr=0)


history1=model.fit_generator(training_set,
                         epochs = 100,
                         validation_data = test_set,
                         callbacks=[checkpoint_callback],
                         steps_per_epoch=len(training_set)/batch_size,
                         validation_steps=len(test_set)/batch_size
                        )

# model.save_weights('/content/gdrive/MyDrive/Colab Notebooks/딥러닝/dl_project/feature_extraction_hyperParemeter_earlyStop.h5')

import matplotlib.pyplot as plt

# 학습 과정에서의 손실을 시각화하는 함수
def plot_loss(history):
    plt.figure(figsize=(12, 6))
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Training and Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

# 학습 과정에서의 스타일 예측 정확도를 시각화하는 함수
def plot_style_accuracy(history):
    plt.figure(figsize=(12, 6))
    plt.plot(history.history['style_accuracy'], label='Training Style Accuracy')
    plt.plot(history.history['val_style_accuracy'], label='Validation Style Accuracy')
    plt.title('Training and Validation Style Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.show()

# 학습 과정에서의 색상 예측 정확도를 시각화하는 함수
def plot_color_accuracy(history):
    plt.figure(figsize=(12, 6))
    plt.plot(history.history['color_accuracy'], label='Training Color Accuracy')
    plt.plot(history.history['val_color_accuracy'], label='Validation Color Accuracy')
    plt.title('Training and Validation Color Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.show()

# 손실 시각화
plot_loss(history1)

# 스타일 예측 정확도 시각화
plot_style_accuracy(history1)

# 색상 예측 정확도 시각화
plot_color_accuracy(history1)

# 학습률 초기값 0.001
vgg_model = VGG16(weights='imagenet', input_shape=(256,256,3), include_top=False)
vgg_model.trainable = False
x = vgg_model.get_layer('block5_pool').output

x = layers.Flatten(name='new_flatten')(x)
branch_a=layers.Dense(units=256)(x)
branch_a=layers.BatchNormalization()(branch_a)
branch_a=layers.Activation('relu')(branch_a)
branch_a=layers.Dropout(0.5)(branch_a)
branch_a=layers.Dense(units=128)(branch_a)
branch_a=layers.BatchNormalization()(branch_a)
branch_a=layers.Dropout(0.5)(branch_a)

style_prediction = layers.Dense(7, activation='softmax', name='style')(branch_a)
color_prediction = layers.Dense(21, activation='softmax', name='color')(branch_a)

model = Model(vgg_model.input,[style_prediction, color_prediction])
model.compile(optimizer=Adam(learning_rate = 0.001), loss={'style':'sparse_categorical_crossentropy','color':'sparse_categorical_crossentropy'}, metrics=['accuracy'])

checkpoint_callback = ModelCheckpoint(
    filepath='feature_extraction_VGG_2.h5',
    monitor='val_loss',
    save_best_only=True,
    mode='min',
    verbose=1
)

# early_stopping_callback = EarlyStopping(
#     monitor='val_loss',
#     patience=10,
#     mode='min',
#     verbose=1
# )

# reduce_learning_rate = ReduceLROnPlateau(
#     monitor='val_loss', factor=0.1, patience=10, verbose=1, mode='auto',
#     min_delta=0.0001, cooldown=0, min_lr=0)


history1=model.fit_generator(training_set,
                         epochs = 100,
                         validation_data = test_set,
                         callbacks=[checkpoint_callback],
                         steps_per_epoch=len(training_set)/batch_size,
                         validation_steps=len(test_set)/batch_size
                        )

# model.save_weights('/content/gdrive/MyDrive/Colab Notebooks/딥러닝/dl_project/feature_extraction_hyperParemeter_earlyStop.h5')

import matplotlib.pyplot as plt

# 학습 과정에서의 손실을 시각화하는 함수
def plot_loss(history):
    plt.figure(figsize=(12, 6))
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Training and Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

# 학습 과정에서의 스타일 예측 정확도를 시각화하는 함수
def plot_style_accuracy(history):
    plt.figure(figsize=(12, 6))
    plt.plot(history.history['style_accuracy'], label='Training Style Accuracy')
    plt.plot(history.history['val_style_accuracy'], label='Validation Style Accuracy')
    plt.title('Training and Validation Style Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.show()

# 학습 과정에서의 색상 예측 정확도를 시각화하는 함수
def plot_color_accuracy(history):
    plt.figure(figsize=(12, 6))
    plt.plot(history.history['color_accuracy'], label='Training Color Accuracy')
    plt.plot(history.history['val_color_accuracy'], label='Validation Color Accuracy')
    plt.title('Training and Validation Color Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.show()

# 손실 시각화
plot_loss(history1)

# 스타일 예측 정확도 시각화
plot_style_accuracy(history1)

# 색상 예측 정확도 시각화
plot_color_accuracy(history1)

# 학습률 초기값 0.0001
vgg_model = VGG16(weights='imagenet', input_shape=(256,256,3), include_top=False)
vgg_model.trainable = False
x = vgg_model.get_layer('block5_pool').output

x = layers.Flatten(name='new_flatten')(x)
branch_a=layers.Dense(units=256)(x)
branch_a=layers.BatchNormalization()(branch_a)
branch_a=layers.Activation('relu')(branch_a)
branch_a=layers.Dropout(0.5)(branch_a)
branch_a=layers.Dense(units=128)(branch_a)
branch_a=layers.BatchNormalization()(branch_a)
branch_a=layers.Dropout(0.5)(branch_a)

style_prediction = layers.Dense(7, activation='softmax', name='style')(branch_a)
color_prediction = layers.Dense(21, activation='softmax', name='color')(branch_a)

model = Model(vgg_model.input,[style_prediction, color_prediction])
model.compile(optimizer=Adam(learning_rate = 0.0001), loss={'style':'sparse_categorical_crossentropy','color':'sparse_categorical_crossentropy'}, metrics=['accuracy'])

checkpoint_callback = ModelCheckpoint(
    filepath='feature_extraction_VGG_2.h5',
    monitor='val_loss',
    save_best_only=True,
    mode='min',
    verbose=1
)

# early_stopping_callback = EarlyStopping(
#     monitor='val_loss',
#     patience=10,
#     mode='min',
#     verbose=1
# )

# reduce_learning_rate = ReduceLROnPlateau(
#     monitor='val_loss', factor=0.1, patience=10, verbose=1, mode='auto',
#     min_delta=0.0001, cooldown=0, min_lr=0)


history1=model.fit_generator(training_set,
                         epochs = 100,
                         validation_data = test_set,
                         callbacks=[checkpoint_callback],
                         steps_per_epoch=len(training_set)/batch_size,
                         validation_steps=len(test_set)/batch_size
                        )

# model.save_weights('/content/gdrive/MyDrive/Colab Notebooks/딥러닝/dl_project/feature_extraction_hyperParemeter_earlyStop.h5')

import matplotlib.pyplot as plt

# 학습 과정에서의 손실을 시각화하는 함수
def plot_loss(history):
    plt.figure(figsize=(12, 6))
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Training and Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

# 학습 과정에서의 스타일 예측 정확도를 시각화하는 함수
def plot_style_accuracy(history):
    plt.figure(figsize=(12, 6))
    plt.plot(history.history['style_accuracy'], label='Training Style Accuracy')
    plt.plot(history.history['val_style_accuracy'], label='Validation Style Accuracy')
    plt.title('Training and Validation Style Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.show()

# 학습 과정에서의 색상 예측 정확도를 시각화하는 함수
def plot_color_accuracy(history):
    plt.figure(figsize=(12, 6))
    plt.plot(history.history['color_accuracy'], label='Training Color Accuracy')
    plt.plot(history.history['val_color_accuracy'], label='Validation Color Accuracy')
    plt.title('Training and Validation Color Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.show()

# 손실 시각화
plot_loss(history1)

# 스타일 예측 정확도 시각화
plot_style_accuracy(history1)

# 색상 예측 정확도 시각화
plot_color_accuracy(history1)

# 학습률 초기값 3e-4
vgg_model = VGG16(weights='imagenet', input_shape=(256,256,3), include_top=False)
vgg_model.trainable = False
x = vgg_model.get_layer('block5_pool').output

x = layers.Flatten(name='new_flatten')(x)
branch_a=layers.Dense(units=256)(x)
branch_a=layers.BatchNormalization()(branch_a)
branch_a=layers.Activation('relu')(branch_a)
branch_a=layers.Dropout(0.5)(branch_a)
branch_a=layers.Dense(units=128)(branch_a)
branch_a=layers.BatchNormalization()(branch_a)
branch_a=layers.Dropout(0.5)(branch_a)

style_prediction = layers.Dense(7, activation='softmax', name='style')(branch_a)
color_prediction = layers.Dense(21, activation='softmax', name='color')(branch_a)

model = Model(vgg_model.input,[style_prediction, color_prediction])
model.compile(optimizer=Adam(learning_rate = 3e-4), loss={'style':'sparse_categorical_crossentropy','color':'sparse_categorical_crossentropy'}, metrics=['accuracy'])

checkpoint_callback = ModelCheckpoint(
    filepath='feature_extraction_VGG_2.h5',
    monitor='val_loss',
    save_best_only=True,
    mode='min',
    verbose=1
)

# early_stopping_callback = EarlyStopping(
#     monitor='val_loss',
#     patience=10,
#     mode='min',
#     verbose=1
# )

# reduce_learning_rate = ReduceLROnPlateau(
#     monitor='val_loss', factor=0.1, patience=10, verbose=1, mode='auto',
#     min_delta=0.0001, cooldown=0, min_lr=0)


history1=model.fit_generator(training_set,
                         epochs = 100,
                         validation_data = test_set,
                         callbacks=[checkpoint_callback],
                         steps_per_epoch=len(training_set)/batch_size,
                         validation_steps=len(test_set)/batch_size
                        )

# model.save_weights('/content/gdrive/MyDrive/Colab Notebooks/딥러닝/dl_project/feature_extraction_hyperParemeter_earlyStop.h5')

import matplotlib.pyplot as plt

# 학습 과정에서의 손실을 시각화하는 함수
def plot_loss(history):
    plt.figure(figsize=(12, 6))
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Training and Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

# 학습 과정에서의 스타일 예측 정확도를 시각화하는 함수
def plot_style_accuracy(history):
    plt.figure(figsize=(12, 6))
    plt.plot(history.history['style_accuracy'], label='Training Style Accuracy')
    plt.plot(history.history['val_style_accuracy'], label='Validation Style Accuracy')
    plt.title('Training and Validation Style Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.show()

# 학습 과정에서의 색상 예측 정확도를 시각화하는 함수
def plot_color_accuracy(history):
    plt.figure(figsize=(12, 6))
    plt.plot(history.history['color_accuracy'], label='Training Color Accuracy')
    plt.plot(history.history['val_color_accuracy'], label='Validation Color Accuracy')
    plt.title('Training and Validation Color Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.show()

# 손실 시각화
plot_loss(history1)

# 스타일 예측 정확도 시각화
plot_style_accuracy(history1)

# 색상 예측 정확도 시각화
plot_color_accuracy(history1)

# 학습률 초기값 0.1, reduce_learning_rate 사용
vgg_model = VGG16(weights='imagenet', input_shape=(256,256,3), include_top=False)
vgg_model.trainable = False
x = vgg_model.get_layer('block5_pool').output

x = layers.Flatten(name='new_flatten')(x)
branch_a=layers.Dense(units=256)(x)
branch_a=layers.BatchNormalization()(branch_a)
branch_a=layers.Activation('relu')(branch_a)
branch_a=layers.Dropout(0.5)(branch_a)
branch_a=layers.Dense(units=128)(branch_a)
branch_a=layers.BatchNormalization()(branch_a)
branch_a=layers.Dropout(0.5)(branch_a)

style_prediction = layers.Dense(7, activation='softmax', name='style')(branch_a)
color_prediction = layers.Dense(21, activation='softmax', name='color')(branch_a)

model = Model(vgg_model.input,[style_prediction, color_prediction])
model.compile(optimizer=Adam(learning_rate = 0.1), loss={'style':'sparse_categorical_crossentropy','color':'sparse_categorical_crossentropy'}, metrics=['accuracy'])

checkpoint_callback = ModelCheckpoint(
    filepath='feature_extraction_VGG_2.h5',
    monitor='val_loss',
    save_best_only=True,
    mode='min',
    verbose=1
)

# early_stopping_callback = EarlyStopping(
#     monitor='val_loss',
#     patience=10,
#     mode='min',
#     verbose=1
# )

reduce_learning_rate = ReduceLROnPlateau(
    monitor='val_loss', factor=0.1, patience=10, verbose=1, mode='auto',
    min_delta=0.0001, cooldown=0, min_lr=0)


history1=model.fit_generator(training_set,
                         epochs = 100,
                         validation_data = test_set,
                         callbacks=[checkpoint_callback, reduce_learning_rate],
                         steps_per_epoch=len(training_set)/batch_size,
                         validation_steps=len(test_set)/batch_size
                        )

# model.save_weights('/content/gdrive/MyDrive/Colab Notebooks/딥러닝/dl_project/feature_extraction_hyperParemeter_earlyStop.h5')

import matplotlib.pyplot as plt

# 학습 과정에서의 손실을 시각화하는 함수
def plot_loss(history):
    plt.figure(figsize=(12, 6))
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Training and Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

# 학습 과정에서의 스타일 예측 정확도를 시각화하는 함수
def plot_style_accuracy(history):
    plt.figure(figsize=(12, 6))
    plt.plot(history.history['style_accuracy'], label='Training Style Accuracy')
    plt.plot(history.history['val_style_accuracy'], label='Validation Style Accuracy')
    plt.title('Training and Validation Style Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.show()

# 학습 과정에서의 색상 예측 정확도를 시각화하는 함수
def plot_color_accuracy(history):
    plt.figure(figsize=(12, 6))
    plt.plot(history.history['color_accuracy'], label='Training Color Accuracy')
    plt.plot(history.history['val_color_accuracy'], label='Validation Color Accuracy')
    plt.title('Training and Validation Color Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.show()

# 손실 시각화
plot_loss(history1)

# 스타일 예측 정확도 시각화
plot_style_accuracy(history1)

# 색상 예측 정확도 시각화
plot_color_accuracy(history1)

# 학습률 초기값 0.1, reduce_learning_rate(하한 3e-5)사용
vgg_model = VGG16(weights='imagenet', input_shape=(256,256,3), include_top=False)
vgg_model.trainable = False
x = vgg_model.get_layer('block5_pool').output

x = layers.Flatten(name='new_flatten')(x)
branch_a=layers.Dense(units=256)(x)
branch_a=layers.BatchNormalization()(branch_a)
branch_a=layers.Activation('relu')(branch_a)
branch_a=layers.Dropout(0.5)(branch_a)
branch_a=layers.Dense(units=128)(branch_a)
branch_a=layers.BatchNormalization()(branch_a)
branch_a=layers.Dropout(0.5)(branch_a)

style_prediction = layers.Dense(7, activation='softmax', name='style')(branch_a)
color_prediction = layers.Dense(21, activation='softmax', name='color')(branch_a)

model = Model(vgg_model.input,[style_prediction, color_prediction])
model.compile(optimizer=Adam(learning_rate = 0.1), loss={'style':'sparse_categorical_crossentropy','color':'sparse_categorical_crossentropy'}, metrics=['accuracy'])

checkpoint_callback = ModelCheckpoint(
    filepath='feature_extraction_VGG_2.h5',
    monitor='val_loss',
    save_best_only=True,
    mode='min',
    verbose=1
)

# early_stopping_callback = EarlyStopping(
#     monitor='val_loss',
#     patience=10,
#     mode='min',
#     verbose=1
# )

reduce_learning_rate = ReduceLROnPlateau(
    monitor='val_loss', factor=0.1, patience=10, verbose=1, mode='auto',
    min_delta=0.0001, cooldown=0, min_lr=3e-5)


history1=model.fit_generator(training_set,
                         epochs = 100,
                         validation_data = test_set,
                         callbacks=[checkpoint_callback, reduce_learning_rate],
                         steps_per_epoch=len(training_set)/batch_size,
                         validation_steps=len(test_set)/batch_size
                        )

# model.save_weights('/content/gdrive/MyDrive/Colab Notebooks/딥러닝/dl_project/feature_extraction_hyperParemeter_earlyStop.h5')

import matplotlib.pyplot as plt

# 학습 과정에서의 손실을 시각화하는 함수
def plot_loss(history):
    plt.figure(figsize=(12, 6))
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Training and Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

# 학습 과정에서의 스타일 예측 정확도를 시각화하는 함수
def plot_style_accuracy(history):
    plt.figure(figsize=(12, 6))
    plt.plot(history.history['style_accuracy'], label='Training Style Accuracy')
    plt.plot(history.history['val_style_accuracy'], label='Validation Style Accuracy')
    plt.title('Training and Validation Style Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.show()

# 학습 과정에서의 색상 예측 정확도를 시각화하는 함수
def plot_color_accuracy(history):
    plt.figure(figsize=(12, 6))
    plt.plot(history.history['color_accuracy'], label='Training Color Accuracy')
    plt.plot(history.history['val_color_accuracy'], label='Validation Color Accuracy')
    plt.title('Training and Validation Color Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.show()

# 손실 시각화
plot_loss(history1)

# 스타일 예측 정확도 시각화
plot_style_accuracy(history1)

# 색상 예측 정확도 시각화
plot_color_accuracy(history1)

"""### 번외 : 모델 변경 (dense layer 추가, drop out 제거)


*   val 성능도 낮지만, train 성능이 너무 낮음 - underfittng


"""

# 학습률 3e-4
vgg_model = VGG16(weights='imagenet', input_shape=(256,256,3), include_top=False)
vgg_model.trainable = False
x = vgg_model.get_layer('block5_pool').output

x = layers.Flatten(name='new_flatten')(x)
branch_a=layers.Dense(units=512)(x)
branch_a=layers.BatchNormalization()(branch_a)
branch_a=layers.Activation('relu')(branch_a)
# branch_a=layers.Dropout(0.5)(branch_a)
branch_a=layers.Dense(units=256)(branch_a)
branch_a=layers.BatchNormalization()(branch_a)
# branch_a=layers.Dropout(0.5)(branch_a)
branch_a=layers.Dense(units=256)(branch_a)
branch_a=layers.BatchNormalization()(branch_a)
branch_a=layers.Activation('relu')(branch_a)
# branch_a=layers.Dropout(0.5)(branch_a)
branch_a=layers.Dense(units=128)(branch_a)
branch_a=layers.BatchNormalization()(branch_a)
# branch_a=layers.Dropout(0.5)(branch_a)

style_prediction = layers.Dense(7, activation='softmax', name='style')(branch_a)
color_prediction = layers.Dense(21, activation='softmax', name='color')(branch_a)

model = Model(vgg_model.input,[style_prediction, color_prediction])
model.compile(optimizer=Adam(learning_rate = 3e-4), loss={'style':'sparse_categorical_crossentropy','color':'sparse_categorical_crossentropy'}, metrics=['accuracy'])

checkpoint_callback = ModelCheckpoint(
    filepath='feature_extraction_VGG_2.h5',
    monitor='val_loss',
    save_best_only=True,
    mode='min',
    verbose=1
)

# early_stopping_callback = EarlyStopping(
#     monitor='val_loss',
#     patience=10,
#     mode='min',
#     verbose=1
# )

# reduce_learning_rate = ReduceLROnPlateau(
#     monitor='val_loss', factor=0.1, patience=10, verbose=1, mode='auto',
#     min_delta=0.0001, cooldown=0, min_lr=0)


history1=model.fit_generator(training_set,
                         epochs = 100,
                         validation_data = test_set,
                         callbacks=[checkpoint_callback],
                         steps_per_epoch=len(training_set)/batch_size,
                         validation_steps=len(test_set)/batch_size
                        )

# model.save_weights('/content/gdrive/MyDrive/Colab Notebooks/딥러닝/dl_project/feature_extraction_hyperParemeter_earlyStop.h5')

import matplotlib.pyplot as plt

# 학습 과정에서의 손실을 시각화하는 함수
def plot_loss(history):
    plt.figure(figsize=(12, 6))
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Training and Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

# 학습 과정에서의 스타일 예측 정확도를 시각화하는 함수
def plot_style_accuracy(history):
    plt.figure(figsize=(12, 6))
    plt.plot(history.history['style_accuracy'], label='Training Style Accuracy')
    plt.plot(history.history['val_style_accuracy'], label='Validation Style Accuracy')
    plt.title('Training and Validation Style Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.show()

# 학습 과정에서의 색상 예측 정확도를 시각화하는 함수
def plot_color_accuracy(history):
    plt.figure(figsize=(12, 6))
    plt.plot(history.history['color_accuracy'], label='Training Color Accuracy')
    plt.plot(history.history['val_color_accuracy'], label='Validation Color Accuracy')
    plt.title('Training and Validation Color Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.show()

# 손실 시각화
plot_loss(history1)

# 스타일 예측 정확도 시각화
plot_style_accuracy(history1)

# 색상 예측 정확도 시각화
plot_color_accuracy(history1)

# 학습률 3e-4, 위에서 drop out 다 뺏더니 val 성능이 낮아짐, dropout 하나 추가
vgg_model = VGG16(weights='imagenet', input_shape=(256,256,3), include_top=False)
vgg_model.trainable = False
x = vgg_model.get_layer('block5_pool').output

x = layers.Flatten(name='new_flatten')(x)
branch_a=layers.Dense(units=512)(x)
branch_a=layers.BatchNormalization()(branch_a)
branch_a=layers.Activation('relu')(branch_a)
# branch_a=layers.Dropout(0.5)(branch_a)
branch_a=layers.Dense(units=256)(branch_a)
branch_a=layers.BatchNormalization()(branch_a)
# branch_a=layers.Dropout(0.5)(branch_a)
branch_a=layers.Dense(units=256)(branch_a)
branch_a=layers.BatchNormalization()(branch_a)
branch_a=layers.Activation('relu')(branch_a)
# branch_a=layers.Dropout(0.5)(branch_a)
branch_a=layers.Dense(units=128)(branch_a)
branch_a=layers.BatchNormalization()(branch_a)
branch_a=layers.Dropout(0.5)(branch_a)

style_prediction = layers.Dense(7, activation='softmax', name='style')(branch_a)
color_prediction = layers.Dense(21, activation='softmax', name='color')(branch_a)

model = Model(vgg_model.input,[style_prediction, color_prediction])
model.compile(optimizer=Adam(learning_rate = 3e-4), loss={'style':'sparse_categorical_crossentropy','color':'sparse_categorical_crossentropy'}, metrics=['accuracy'])

checkpoint_callback = ModelCheckpoint(
    filepath='feature_extraction_VGG_2.h5',
    monitor='val_loss',
    save_best_only=True,
    mode='min',
    verbose=1
)

# early_stopping_callback = EarlyStopping(
#     monitor='val_loss',
#     patience=10,
#     mode='min',
#     verbose=1
# )

# reduce_learning_rate = ReduceLROnPlateau(
#     monitor='val_loss', factor=0.1, patience=10, verbose=1, mode='auto',
#     min_delta=0.0001, cooldown=0, min_lr=0)


history1=model.fit_generator(training_set,
                         epochs = 100,
                         validation_data = test_set,
                         callbacks=[checkpoint_callback],
                         steps_per_epoch=len(training_set)/batch_size,
                         validation_steps=len(test_set)/batch_size
                        )

# model.save_weights('/content/gdrive/MyDrive/Colab Notebooks/딥러닝/dl_project/feature_extraction_hyperParemeter_earlyStop.h5')

import matplotlib.pyplot as plt

# 학습 과정에서의 손실을 시각화하는 함수
def plot_loss(history):
    plt.figure(figsize=(12, 6))
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Training and Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

# 학습 과정에서의 스타일 예측 정확도를 시각화하는 함수
def plot_style_accuracy(history):
    plt.figure(figsize=(12, 6))
    plt.plot(history.history['style_accuracy'], label='Training Style Accuracy')
    plt.plot(history.history['val_style_accuracy'], label='Validation Style Accuracy')
    plt.title('Training and Validation Style Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.show()

# 학습 과정에서의 색상 예측 정확도를 시각화하는 함수
def plot_color_accuracy(history):
    plt.figure(figsize=(12, 6))
    plt.plot(history.history['color_accuracy'], label='Training Color Accuracy')
    plt.plot(history.history['val_color_accuracy'], label='Validation Color Accuracy')
    plt.title('Training and Validation Color Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.show()

# 손실 시각화
plot_loss(history1)

# 스타일 예측 정확도 시각화
plot_style_accuracy(history1)

# 색상 예측 정확도 시각화
plot_color_accuracy(history1)

# 학습률 초기값 3e-4, 모델 모양 변경(혹시 몰라서 해 봄..)
vgg_model = VGG16(weights='imagenet', input_shape=(256,256,3), include_top=False)
vgg_model.trainable = False
x = vgg_model.get_layer('block5_pool').output

x1 = layers.Flatten(name='flatten1')(x)
branch_a=layers.Dense(units=512)(x1)
branch_a=layers.BatchNormalization()(branch_a)
branch_a=layers.Activation('relu')(branch_a)
# branch_a=layers.Dropout(0.5)(branch_a)
branch_a=layers.Dense(units=256)(branch_a)
branch_a=layers.BatchNormalization()(branch_a)
# branch_a=layers.Dropout(0.5)(branch_a)
branch_a=layers.Dense(units=256)(branch_a)
branch_a=layers.BatchNormalization()(branch_a)
branch_a=layers.Activation('relu')(branch_a)
# branch_a=layers.Dropout(0.5)(branch_a)
branch_a=layers.Dense(units=128)(branch_a)
branch_a=layers.BatchNormalization()(branch_a)
# branch_a=layers.Dropout(0.5)(branch_a)

x2 = layers.Flatten(name='flatten2')(x)
branch_b=layers.Dense(units=512)(x2)
branch_b=layers.BatchNormalization()(branch_b)
branch_b=layers.Activation('relu')(branch_b)
# branch_b=layers.Dropout(0.5)(branch_b)
branch_b=layers.Dense(units=256)(branch_b)
branch_b=layers.BatchNormalization()(branch_b)
# branch_b=layers.Dropout(0.5)(branch_b)
branch_b=layers.Dense(units=256)(branch_b)
branch_b=layers.BatchNormalization()(branch_b)
branch_b=layers.Activation('relu')(branch_b)
# branch_b=layers.Dropout(0.5)(branch_b)
branch_b=layers.Dense(units=128)(branch_b)
branch_b=layers.BatchNormalization()(branch_b)
# branch_b=layers.Dropout(0.5)(branch_b)

style_prediction = layers.Dense(7, activation='softmax', name='style')(branch_a)
color_prediction = layers.Dense(21, activation='softmax', name='color')(branch_b)

model = Model(vgg_model.input,[style_prediction, color_prediction])
model.compile(optimizer=Adam(learning_rate = 0.001), loss={'style':'sparse_categorical_crossentropy','color':'sparse_categorical_crossentropy'}, metrics=['accuracy'])

checkpoint_callback = ModelCheckpoint(
    filepath='feature_extraction_VGG_2.h5',
    monitor='val_loss',
    save_best_only=True,
    mode='min',
    verbose=1
)

# early_stopping_callback = EarlyStopping(
#     monitor='val_loss',
#     patience=10,
#     mode='min',
#     verbose=1
# )

# reduce_learning_rate = ReduceLROnPlateau(
#     monitor='val_loss', factor=0.1, patience=10, verbose=1, mode='auto',
#     min_delta=0.0001, cooldown=0, min_lr=0)


history1=model.fit_generator(training_set,
                         epochs = 100,
                         validation_data = test_set,
                         callbacks=[checkpoint_callback, reduce_learning_rate],
                         steps_per_epoch=len(training_set)/batch_size,
                         validation_steps=len(test_set)/batch_size
                        )

# model.save_weights('/content/gdrive/MyDrive/Colab Notebooks/딥러닝/dl_project/feature_extraction_hyperParemeter_earlyStop.h5')

import matplotlib.pyplot as plt

# 학습 과정에서의 손실을 시각화하는 함수
def plot_loss(history):
    plt.figure(figsize=(12, 6))
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Training and Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

# 학습 과정에서의 스타일 예측 정확도를 시각화하는 함수
def plot_style_accuracy(history):
    plt.figure(figsize=(12, 6))
    plt.plot(history.history['style_accuracy'], label='Training Style Accuracy')
    plt.plot(history.history['val_style_accuracy'], label='Validation Style Accuracy')
    plt.title('Training and Validation Style Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.show()

# 학습 과정에서의 색상 예측 정확도를 시각화하는 함수
def plot_color_accuracy(history):
    plt.figure(figsize=(12, 6))
    plt.plot(history.history['color_accuracy'], label='Training Color Accuracy')
    plt.plot(history.history['val_color_accuracy'], label='Validation Color Accuracy')
    plt.title('Training and Validation Color Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.show()

# 손실 시각화
plot_loss(history1)

# 스타일 예측 정확도 시각화
plot_style_accuracy(history1)

# 색상 예측 정확도 시각화
plot_color_accuracy(history1)

"""### 추가 데이터"""

# 학습률 3e-4
vgg_model = VGG16(weights='imagenet', input_shape=(256,256,3), include_top=False)
vgg_model.trainable = False
x = vgg_model.get_layer('block5_pool').output

x = layers.Flatten(name='new_flatten')(x)
branch_a=layers.Dense(units=512)(x)
branch_a=layers.BatchNormalization()(branch_a)
branch_a=layers.Activation('relu')(branch_a)
# branch_a=layers.Dropout(0.5)(branch_a)
branch_a=layers.Dense(units=256)(branch_a)
branch_a=layers.BatchNormalization()(branch_a)
# branch_a=layers.Dropout(0.5)(branch_a)
branch_a=layers.Dense(units=256)(branch_a)
branch_a=layers.BatchNormalization()(branch_a)
branch_a=layers.Activation('relu')(branch_a)
# branch_a=layers.Dropout(0.5)(branch_a)
branch_a=layers.Dense(units=128)(branch_a)
branch_a=layers.BatchNormalization()(branch_a)
# branch_a=layers.Dropout(0.5)(branch_a)

style_prediction = layers.Dense(7, activation='softmax', name='style')(branch_a)
color_prediction = layers.Dense(21, activation='softmax', name='color')(branch_a)

model = Model(vgg_model.input,[style_prediction, color_prediction])
model.compile(optimizer=Adam(learning_rate = 3e-4), loss={'style':'sparse_categorical_crossentropy','color':'sparse_categorical_crossentropy'}, metrics=['accuracy'])

checkpoint_callback = ModelCheckpoint(
    filepath='feature_extraction_VGG_2.h5',
    monitor='val_loss',
    save_best_only=True,
    mode='min',
    verbose=1
)

# early_stopping_callback = EarlyStopping(
#     monitor='val_loss',
#     patience=10,
#     mode='min',
#     verbose=1
# )

# reduce_learning_rate = ReduceLROnPlateau(
#     monitor='val_loss', factor=0.1, patience=10, verbose=1, mode='auto',
#     min_delta=0.0001, cooldown=0, min_lr=0)


history1=model.fit_generator(training_set,
                         epochs = 100,
                         validation_data = test_set,
                         callbacks=[checkpoint_callback],
                         steps_per_epoch=len(training_set)/batch_size,
                         validation_steps=len(test_set)/batch_size
                        )

# model.save_weights('/content/gdrive/MyDrive/Colab Notebooks/딥러닝/dl_project/feature_extraction_hyperParemeter_earlyStop.h5')

import matplotlib.pyplot as plt

# 학습 과정에서의 손실을 시각화하는 함수
def plot_loss(history):
    plt.figure(figsize=(12, 6))
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Training and Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

# 학습 과정에서의 스타일 예측 정확도를 시각화하는 함수
def plot_style_accuracy(history):
    plt.figure(figsize=(12, 6))
    plt.plot(history.history['style_accuracy'], label='Training Style Accuracy')
    plt.plot(history.history['val_style_accuracy'], label='Validation Style Accuracy')
    plt.title('Training and Validation Style Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.show()

# 학습 과정에서의 색상 예측 정확도를 시각화하는 함수
def plot_color_accuracy(history):
    plt.figure(figsize=(12, 6))
    plt.plot(history.history['color_accuracy'], label='Training Color Accuracy')
    plt.plot(history.history['val_color_accuracy'], label='Validation Color Accuracy')
    plt.title('Training and Validation Color Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.show()

# 손실 시각화
plot_loss(history1)

# 스타일 예측 정확도 시각화
plot_style_accuracy(history1)

# 색상 예측 정확도 시각화
plot_color_accuracy(history1)

"""### 정리

*   trial 1에서 early stoping을 사용하지 않는 게 성능이 높은 걸 확인, 또한 epoch을 100으로 했을 때 train loss가 아직 수렴하지 않은 점을 통해서 underfitting으로 추정, 레이어와 노드 수를 추가, dropout 레이어 배제
*   trial 2에서 fine tuning과 end to end 모두 pretrain을 이용한 모델보다 성능이 낮으므로 pretrain 이용(이유 추정 및 설명 필요)
*   ReduceLROnPlateau 사용하니, 성능이 더 떨어짐, 3e-4에서 최고 성능


"""

